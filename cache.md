KV cache 的“淘汰/替换”除了 LRU，其实有一整套可选策略；而在 LLM 场景里，还会叠加“前缀树/块引用计数/分层(offload)”这些结构性约束，所以经常不是纯粹的通用缓存算法，而是“算法 + 结构感知”的组合。

下面按你在推理系统里最可能用到的类别来列。

1) 最常见的“基础替换策略”（工程里一行配置就能切）

这些基本都能在现成系统里直接用到，例如 Redis 的缓存策略对比也经常被拿来类比。 ￼
	•	FIFO：先进先出。实现最简单；对“突发一次性长输入”容易把热数据挤掉（缓存污染）。
	•	MRU：淘汰“最近用过的”。在某些顺序扫描（只会用一次、很快不再访问）工作负载上比 LRU 更好；对对话型/重复访问型一般更差。LMCache 就支持 MRU。 ￼
	•	LFU：最不经常用的先淘汰。适合有明显热点、热点长期稳定的负载；但容易“记住老热点”导致新热点进不来，通常要配合“衰减/aging”。Dynamo 文档也明确提到可以用 LRU/LFU。 ￼
	•	TTL / 过期策略（严格说是“失效”而不是淘汰）：对“前缀缓存”非常常见：比如只缓存 10 分钟的热门系统提示词/文档前缀，减少跨租户风险与版本不一致问题（见下文安全/一致性）。

现成支持例子：LMCache 目前公开文档写了支持 LRU / MRU / LFU / FIFO。 ￼

2) 更“抗污染”的通用算法（比 LRU/LFU 更稳）

这些通常用在“热点会变、长尾多、扫描多”的环境里，优势是命中率更稳，但实现更复杂：
	•	2Q：把缓存分成“短期队列 + 长期队列”，先让新进来的对象在短期队列“试用”，只有被再次访问才进长期队列；能明显降低一次性扫描造成的污染（比纯 LRU 稳很多）。HotPrefix 论文里就拿 2Q 作为对比基线之一。 ￼
	•	ARC（Adaptive Replacement Cache）：在 LRU（recency）和 LFU（frequency）之间自适应调整权重；对负载变化很强的线上服务很友好。
	•	LRU-K：看“最近 K 次访问”而不是最近 1 次，能区分“偶然命中”和“真正重复访问”。
	•	CLOCK / CLOCK-Pro：用环形指针近似 LRU（以及更高级的自适应版本），开销更小，常见于操作系统页缓存。

这些策略的共同点：更像操作系统页缓存的思路，适合 KV block / page 粒度的缓存管理。

3) LLM 前缀缓存特有：树/块感知的淘汰（不是简单 LRU）

当你做的是“前缀共享”（prefix caching / radix tree / paged blocks），最关键的差异是：
	•	KV 块往往构成前缀树（共享前缀的分叉结构）
	•	块可能有引用计数（多个请求共享同一段前缀）
	•	真正可淘汰的一般是引用计数为 0 的叶子节点（leaf blocks）

例如 vLLM 的自动前缀缓存文档描述的淘汰策略本质上就是：优先淘汰“引用计数为 0 且最近最少使用的叶子”，并在 tie-break 时更偏向“更深的叶子”。 ￼
这类“结构感知淘汰”在 LLM 场景里比你在 KV-store 里见到的 LRU 更重要，因为它直接决定“共享前缀”能否长期留住。

此外还有研究走向“热度感知调度/淘汰”，比如 HotPrefix 这种把“前缀热度”纳入决策的方案（论文里对比了 LRU/LFU/FIFO/2Q 等）。 ￼

4) 分层缓存（G1/G2/G3）里的“放置 + 迁移”策略

你之前问的 G1/G2/G3（HBM/DRAM/SSD）一旦引入，策略通常变成两件事：
	1.	哪一层放什么（placement）
	2.	什么时候搬（migration / prefetch / write-back）

这时会出现“跨层版本”的算法组合，例如：
	•	在 G1 用 LRU/2Q（保证热点驻留 HBM）
	•	在 G2 用 LFU/ARC（抓住更稳定的复用前缀）
	•	在 G3 只存“可复用、且回流价值大”的前缀（可能用更严格的 admission policy：例如要命中两次才准入 SSD）
	•	预取（prefetch）：根据路由器判断即将命中的前缀，提前从 G2/G3 搬回 G1，降低 TTFT 抖动

在 NVIDIA 的 Dynamo 文档中，明确提到系统会应用淘汰策略（如 LRU/LFU），并且也可能把不常用块 offload 到 CPU 内存（这就是“分层迁移”的一部分）。 ￼

5) “优先级/Pinning/租户隔离”——LLM 工程里经常比算法更关键

很多时候你会发现：算法不是瓶颈，策略约束才是。
	•	Priority-based eviction（优先级淘汰）：把“系统提示词前缀”“高价值会话”“付费租户”等设置更高保留优先级；甚至能给优先级设置持续时间。TensorRT-LLM Executor API 就引入了“priority + duration”来影响块淘汰。 ￼
	•	Pin / Lock：把某些前缀块锁住不让淘汰（例如常驻 system prompt、RAG 里热门文档的前缀）。
	•	多租户安全约束：共享 KV cache 会引入侧信道/信息泄露风险，因此很多系统会对“跨租户共享”做限制，或采用 TTL/分区（per-tenant cache）等。 ￼
这会直接改变你能用的算法空间：你可能必须“分租户 LRU”而不是“全局 LFU”。

⸻

怎么选（给你一个快速落地的决策）
	•	你做的是前缀共享 + radix/paged blocks：优先上“叶子 + refcount 感知的 LRU”，再考虑 2Q/ARC 做增强。 ￼
	•	工作负载有大量一次性长输入（扫描污染）：2Q / ARC 往往比纯 LRU 稳。
	•	热点非常稳定（比如固定系统提示词、固定热门文档）：LFU（配 aging）+ pin/priority 很有效。 ￼
	•	多租户/安全敏感：优先做分区（tenant隔离）+ TTL，再谈全局最优命中率。 ￼

如果你愿意，我可以把“LLM 推理里的 KV cache 访问模式”再细分成几类（聊天/RAG/代码补全/批处理离线）并给出每类最推荐的淘汰 + 准入(admission) + 迁移组合，以及它们对延迟尾部（P95/P99）和容量占用的影响。
